# CLEANING-ROBOT
Computing Optimal Policies for Unknown MDPs using Q-learning and Rapidly exploring Random Trees for a Cleaning Robot

Objective: To compute the optimal policy
for unknown MDP using the Q learning
and RRT planning Algorithm and achieve
the better performance of cleaning the
room with an optimal policy.

The Robot has the following features.
1. It should prohibit the obstacles.
2. Robot has memory to store the spots being
visited and being sensed with relative to
the starting point
3. It could able to sense the walls

Q-learning is an algorithm that can be used
to solve some types of RL problems. we
can represent action-value functions q(s, a)
as a table with |S| rows indexed by states
s ∈ S and |A| columns indexed by actions
a ∈ A.

This algorithm can
be interpreted as a variation of value
iteration, which identifies the iteration
counter in value iteration algorithm with
a real-time index.
The Planning algorithm with the QLearning
algorithm allows to learn the
optimal behavior while the AI system
works (takes actions, receives rewards and
experiences state transitions to new states)
in its environment.

Q-Learning ALGORITHM (deterministic
MDP)
• Input: discount factor γ ∈ (0, 1), start time
t
• Initialize: set qt−1(s,a)= 0 for every hs,
ai∈S × A, determine current state st
• Step 1: take some action at ∈ A
• Step 2: wait until new state st+1 is
reached and reward R(st, at) is received
• Step 3:– update action-value function at
particular state-action pair hst, ati ∈ S × A
qt(st, at) = R(st, at) + γ max a0∈Aqt−1(st+1,
a0) copy action-value estimate for all other
state-action pairs, i.e.,qt(s, a) = qt−1(s, a) for
all hs, ai ∈ S × A \ {hst, ati}
• Step 4: if not converged, update time
index t := t + 1 and go to Step 1
• Output: estimate Q(s, a) = qt(s, a) of
optimal action-value function q∗
Thus, after a sufficient number of epochs
the function qt(s, a) generated by above
algorithm is an accurate approximation of
the optimal action-value function q∗(s, a)
of the unknown MDP. The optimal policy
for the MDP by acting greedily, is
πˆ(s) := argmax a∈A Q(s, a)
Most of the work is done by the
moveAgent() and step1 function, which
computes the values of the Q matrix. Qlearning
is iterative, so a maximum
number of iterations, (i.e. Steps and
epochs) is set. Q-learning has two
important parameters, the learning rate
and gamma. The Larger values of the
learning rate increase the influence of both
current rewards and future rewards
(explore) at the expense of past rewards
(exploit). The value of gamma, also called
Series1
Series8
-2E+09
-1E+09
0
1E+09
2E+09
1 3 5 7 9 11 13
Qtable
Series1 Series2 Series3 Series4
Series5 Series6 Series7 Series8
the discount factor, influences the
importance of future rewards. These
values must be determined by trial and
error but using 0.5 is typically a good
starting place.Q-Learning ALGORITHM (deterministic
MDP)
• Input: discount factor γ ∈ (0, 1), start time
t
• Initialize: set qt−1(s,a)= 0 for every hs,
ai∈S × A, determine current state st
• Step 1: take some action at ∈ A
• Step 2: wait until new state st+1 is
reached and reward R(st, at) is received
• Step 3:– update action-value function at
particular state-action pair hst, ati ∈ S × A
qt(st, at) = R(st, at) + γ max a0∈Aqt−1(st+1,
a0) copy action-value estimate for all other
state-action pairs, i.e.,qt(s, a) = qt−1(s, a) for
all hs, ai ∈ S × A \ {hst, ati}
• Step 4: if not converged, update time
index t := t + 1 and go to Step 1
• Output: estimate Q(s, a) = qt(s, a) of
optimal action-value function q∗
Thus, after a sufficient number of epochs
the function qt(s, a) generated by above
algorithm is an accurate approximation of
the optimal action-value function q∗(s, a)
of the unknown MDP. The optimal policy
for the MDP by acting greedily, is
πˆ(s) := argmax a∈A Q(s, a)
Most of the work is done by the
moveAgent() and step1 function, which
computes the values of the Q matrix. Qlearning
is iterative, so a maximum
number of iterations, (i.e. Steps and
epochs) is set. Q-learning has two
important parameters, the learning rate
and gamma. The Larger values of the
learning rate increase the influence of both
current rewards and future rewards
(explore) at the expense of past rewards
(exploit). The value of gamma, also called
the discount factor, influences the
importance of future rewards. These
values must be determined by trial and
error but using 0.5 is typically a good
starting place.

Computing the Q Matrix:
The heart of the demo program is function
QLearner(), which computes the Q matrix.
The code for QLearner() is presented in
code. The Q-learning update equation is
based on a clever idea called the Bellman
equation.

Walking the Q Matrix:
After the Q matrix has been computed, it
can be used by function step1(). The
challenge is to perform a sequence of
actions that finds a long-term reward,
without explicitly defining rules. Qlearning
can be useful when you can safely
train a physical system with many trials,
such as training a vacuum cleaning robot
how to navigate through a room.
2. Integrating Random trees planning
algorithm to create efficient way of
cleaning in a smaller number of steps. The
planning algorithm was clearly shown in
below algorithm

Planning Algorithm:
Robotics-Planning Includes mechanism
design, dynamical system modeling,
feedback control, sensor design, computer
vision, inverse kinematics, and humanoid
robotics. This is a part of big picture of
artificial intelligence. A fundamental need
in robotics is to have algorithms that
convert high-level specifications of tasks
from humans into low-level descriptions
of how to move.

Start {states: 16*16}
2. Initial action will be taken according to the
declared enumerated action from,
Action= {North, South, East, West}
3. Mapping: As it maps the moves it has
taken. The moves are depicted in the
console.
Learning phenomenon introduces the Q
value which can improve the cleaning
performance.
4. Initialize Q value [16][16][4]; rows:
columns: action
5. Initialize map const
6. Create another map to update, which is not
constant.
7. Calculate Q learn function
8. Update the coordinates and Update the
map= constant map
9. Reset
10. Reward the steps
11. Take a step based on the Q value
enumerated.
12. double a=0.7;// learning parameter
13. double g = 0.9;// discount factor, gamma
14. {Q value, Bellman equation}
Important Q definitions:
Here D is the enumerating factor for the
actions to be taken based on the Q
learning.
int D = distance(Qtable[agentX][agentY],
max_element(Qtable[agentX][agentY],
Qtable[agentX][agentY] + 4));
Qtable[oldX][oldY][D] =
Qtable[oldX][oldY][D] + (a)* reward + g *
maxQ[agentX][agentY][D];
15. By Following the greedy parameter, the
exploration is done using planning
algorithm at sometimes.
16. Embedding the planning algorithm will
make it a very powerful cleaning robot
with minimum number of steps.
Software Agent which can learn by
reinforcement learning:
Environment: The environment
State: There are 16*16 states in the maze
Action: {North, South, East, West}
